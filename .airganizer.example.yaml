# Airganizer Configuration File
# Edit this file to configure your AI provider and settings
#
# This file supports comments! Use # to add notes and documentation.

# ============================================================================
# AI Provider Selection
# ============================================================================
# Choose which AI provider to use: 'openai', 'anthropic', or 'ollama'
# - openai: Cloud-based, requires API key, best quality
# - anthropic: Cloud-based, requires API key, good quality
# - ollama: Local, free, works offline, supports GPU/Metal
ai_provider: ollama

# ============================================================================
# Chunking Configuration
# ============================================================================
# Maximum size of each chunk in characters when processing large directories
# 
# IMPORTANT: For large datasets (100GB+), use MUCH larger chunks!
# 
# Context window limits (approximate):
# - GPT-4 Turbo: 128K tokens (~400K chars) - Use chunk_size: 300000-400000
# - Claude 3.5 Sonnet: 200K tokens (~600K chars) - Use chunk_size: 500000-600000
# - Gemini 1.5 Pro: 1M tokens (~3M chars) - Use chunk_size: 2000000+
# - Ollama qwen2.5:32b: 32K tokens (~100K chars) - Use chunk_size: 80000-100000
# - Ollama llama3.2: 128K tokens (~400K chars) - Use chunk_size: 300000-400000
#
# Recommended settings by dataset size:
# - Small (< 1GB): 4000-8000
# - Medium (1-10GB): 20000-50000
# - Large (10-100GB): 100000-200000
# - Very Large (100GB+): 300000-600000 (requires cloud AI with large context)
#
# Example: 650GB dataset with 560K chunks @ 4000 = 16 days
#          650GB dataset with 4K chunks @ 500000 = ~3 hours (125x faster!)
chunk_size: 4000

# ============================================================================
# Output Format Configuration
# ============================================================================
# Format for sending file tree to AI provider
# Options:
# - pathlist: Most compact, one file path per line (RECOMMENDED)
# - compact: Medium size, shows directory hierarchy with indentation
# - json: Full JSON tree structure (most verbose)
#
# For large datasets, use 'pathlist' or 'compact' to save tokens
format: pathlist

# ============================================================================
# AI Model Behavior Configuration
# ============================================================================
# Temperature controls randomness/creativity of AI responses (0.0-1.0)
# - 0.0: Deterministic, consistent output
# - 0.3: RECOMMENDED - Balanced consistency with some variation
# - 0.7: More creative but less predictable
# - 1.0: Maximum creativity/randomness
temperature: 0.3

# System prompt sent to AI model
# Customize this to change how the AI organizes your files
# The AI can reorganize and restructure categories as it processes new chunks
system_prompt: 'You are an expert file organizer. Generate clean, logical directory structures in JSON format. You can reorganize and restructure categories as you process new data.'

# ============================================================================
# OpenAI Configuration
# ============================================================================
openai:
  # Get your API key from: https://platform.openai.com/api-keys
  # Or set environment variable: OPENAI_API_KEY
  api_key: ''
  
  # Model to use. Options:
  # - gpt-4-turbo: RECOMMENDED for large datasets (128K context, fast)
  # - gpt-4: Best quality but slower (8K context)
  # - gpt-3.5-turbo: Fast and cheap but limited context (16K)
  #
  # For 650GB+: Use gpt-4-turbo with chunk_size: 300000-400000
  model: gpt-4-turbo
  
  # Maximum tokens to generate in response
  # Higher values allow longer, more detailed structures
  # Range: 1-4096 for most models, up to 128K for turbo
  max_tokens: 4096
  
  # Response format type
  # - json_object: Force JSON output (RECOMMENDED)
  # - text: Allow text responses (may require parsing)
  response_format: json_object

# ============================================================================
# Anthropic (Claude) Configuration
# ============================================================================
anthropic:
  # Get your API key from: https://console.anthropic.com/
  # Or set environment variable: ANTHROPIC_API_KEY
  api_key: ''
  
  # Model to use. Options:
  # - claude-3-5-sonnet-20241022: BEST for large datasets (200K context, excellent quality)
  # - claude-3-opus-20240229: Most capable but slower (200K context)
  # - claude-3-sonnet-20240229: Balanced (200K context)
  #
  # For 650GB+: Use claude-3-5-sonnet with chunk_size: 500000-600000
  model: claude-3-5-sonnet-20241022
  
  # Maximum tokens to generate in response
  # Higher values allow longer, more detailed structures
  # Range: 1-4096 (Anthropic default limit)
  max_tokens: 4096

# ============================================================================
# Ollama (Local AI) Configuration
# ============================================================================
ollama:
  # Model to use. Popular options with their context windows:
  # - qwen2.5:32b - BEST for large datasets (32K context, excellent quality)
  # - llama3.2:90b - High quality (128K context, needs 64GB+ RAM)
  # - llama3.2:70b - Great quality (128K context, needs 40GB+ RAM)
  # - mistral:7b - Balanced (32K context, 4GB RAM)
  # - llama3.2:11b - Good quality (128K context, 7GB RAM)
  # - llama3.2:3b - Fast, light (128K context, 2GB RAM)
  # 
  # For 650GB+ datasets on local hardware:
  # - M4 Mac Mini (16GB): qwen2.5:7b with chunk_size: 80000
  # - M4 Mac Mini (32GB): qwen2.5:32b with chunk_size: 100000
  # - High-end workstation: llama3.2:70b with chunk_size: 300000
  #
  # Install models with: ollama pull <model-name>
  # List models with: ollama list
  model: qwen2.5:32b
  
  # Base URL for Ollama server
  # Default is localhost. Change if running on a different machine.
  base_url: http://localhost:11434
  
  # Maximum tokens to generate in response
  # Controls output length. Higher values = longer structures
  # Range: Depends on model, typically 512-4096
  # Note: Generation stops automatically when complete
  num_predict: 2048

# ============================================================================
# Notes
# ============================================================================
# - Environment variables override config file settings
# - All AI providers automatically use GPU when available
# - All parameters can be configured here - no hardcoded values in the app!
# - Ollama uses Metal on Mac, CUDA on NVIDIA, ROCm on AMD
# - You can test connection with: python -m airganizer.main . --organize --debug

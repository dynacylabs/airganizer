# Airganizer Configuration File
# Edit this file to configure your AI provider and settings
#
# This file supports comments! Use # to add notes and documentation.

# ============================================================================
# AI Provider Selection
# ============================================================================
# Choose which AI provider to use: 'openai', 'anthropic', or 'ollama'
# - openai: Cloud-based, requires API key, best quality
# - anthropic: Cloud-based, requires API key, good quality
# - ollama: Local, free, works offline, supports GPU/Metal
ai_provider: ollama

# ============================================================================
# Chunking Configuration
# ============================================================================
# Maximum size of each chunk in characters when processing large directories
# 
# IMPORTANT: For large datasets (100GB+), use MUCH larger chunks!
# 
# Context window limits (approximate):
# - GPT-4 Turbo: 128K tokens (~400K chars) - Use chunk_size: 300000-400000
# - Claude 3.5 Sonnet: 200K tokens (~600K chars) - Use chunk_size: 500000-600000
# - Gemini 1.5 Pro: 1M tokens (~3M chars) - Use chunk_size: 2000000+
# - Ollama qwen2.5:32b: 32K tokens (~100K chars) - Use chunk_size: 80000-100000
# - Ollama llama3.2: 128K tokens (~400K chars) - Use chunk_size: 300000-400000
#
# Recommended settings by dataset size:
# - Small (< 1GB): 4000-8000
# - Medium (1-10GB): 20000-50000
# - Large (10-100GB): 100000-200000
# - Very Large (100GB+): 300000-600000 (requires cloud AI with large context)
#
# Example: 650GB dataset with 560K chunks @ 4000 = 16 days
#          650GB dataset with 4K chunks @ 500000 = ~3 hours (125x faster!)
chunk_size: 4000

# ============================================================================
# OpenAI Configuration
# ============================================================================
openai:
  # Get your API key from: https://platform.openai.com/api-keys
  # Or set environment variable: OPENAI_API_KEY
  api_key: ''
  
  # Model to use. Options:
  # - gpt-4: Best quality, slower, more expensive
  # - gpt-4-turbo: Fast and capable
  # - gpt-3.5-turbo: Fast and cheap
  model: gpt-4

# ============================================================================
# Anthropic (Claude) Configuration
# ============================================================================
anthropic:
  # Get your API key from: https://console.anthropic.com/
  # Or set environment variable: ANTHROPIC_API_KEY
  api_key: ''
  
  # Model to use. Options:
  # - claude-3-5-sonnet-20241022: Best for complex tasks
  # - claude-3-opus-20240229: Most capable
  # - claude-3-sonnet-20240229: Balanced
  model: claude-3-5-sonnet-20241022

# ============================================================================
# Ollama (Local AI) Configuration
# ============================================================================
ollama:
  # Model to use. Popular options:
  # - llama3.2:3b - Fast, light (2GB RAM)
  # - mistral:7b - Balanced (4GB RAM) - RECOMMENDED
  # - llama3.2:11b - Better quality (7GB RAM)
  # - llama3.2:70b - Best quality (40GB RAM)
  # 
  # Install models with: ollama pull <model-name>
  # List models with: ollama list
  model: llama2
  
  # Base URL for Ollama server
  # Default is localhost. Change if running on a different machine.
  base_url: http://localhost:11434

# ============================================================================
# Notes
# ============================================================================
# - Environment variables override config file settings
# - All AI providers automatically use GPU when available
# - Ollama uses Metal on Mac, CUDA on NVIDIA, ROCm on AMD
# - You can test connection with: python -m airganizer.main . --organize --debug

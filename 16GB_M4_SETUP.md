# Optimal Settings for 16GB M4 Mac Mini

## Configuration for 650GB Dataset

### .airganizer.yaml

```yaml
# Airganizer Configuration File
# Optimized for 16GB M4 Mac Mini processing 650GB dataset

# AI provider - use local Ollama
ai_provider: ollama

# Chunk size optimized for 16GB RAM
# 80K chars = ~11x faster than default 4K
chunk_size: 80000

ollama:
  # qwen2.5:7b is the best model for 16GB RAM
  # - Only ~4.7GB model size
  # - 32K token context (can handle 80K char chunks)
  # - Excellent quality for organization tasks
  # - Fast on M4 with Metal acceleration
  model: qwen2.5:7b
  base_url: http://localhost:11434
```

## Setup Commands

```bash
# 1. Install Ollama (if not already installed)
brew install ollama

# 2. Start Ollama service
ollama serve

# 3. In another terminal, pull the model (~4.7GB download)
ollama pull qwen2.5:7b

# 4. Verify it's working
ollama run qwen2.5:7b "Hello, can you help organize files?"

# 5. Install Python dependencies
pip install pyyaml ollama

# 6. Create/update Airganizer config
python -m airganizer.main . --init-config

# 7. Edit the config to match the settings above
nano .airganizer.yaml
```

## Running on Your 650GB Dataset

```bash
# Make sure Ollama is running
ollama serve

# In another terminal, run Airganizer with debug mode
python -m airganizer.main /path/to/your/650gb/data \
  --organize \
  --debug \
  --output organized_structure.json
```

## Performance Expectations

### Your Current Setup:
- Chunk size: 4,000 chars
- Total chunks: 560,000
- Time per chunk: ~2.5s
- **Total time: ~16 days** âŒ

### Optimized Setup (16GB M4):
- Chunk size: 80,000 chars (20x larger!)
- Total chunks: ~62,500
- Time per chunk: ~5s (faster processing, but bigger chunks)
- **Total time: ~3.6 days** âœ…

**Result: 4.4x faster processing!**

## Memory Usage

With these settings:
- Ollama model: ~4.7GB
- Model runtime: ~6-8GB
- System: ~2-3GB
- Airganizer: ~1GB
- **Total: ~14GB** (fits comfortably in 16GB)

Leaves ~2GB for other processes.

## Monitor Progress

### Check GPU Usage:
```bash
# In another terminal
sudo powermetrics --samplers gpu_power -i 1000 -n 1
```

Or use Activity Monitor:
1. Open Activity Monitor
2. Window â†’ GPU History
3. You should see consistent GPU activity

### Watch Progress File:
```bash
# Monitor the structure as it's being built
watch -n 10 'cat /tmp/airganizer_progress_*.json | python -m json.tool | head -40'
```

### Debug Output Shows:
```
[DEBUG] Created temporary progress file: /tmp/airganizer_progress_xxx.json
[DEBUG] Chunk size: 80000 characters
[DEBUG] Chunk size source: config file
[DEBUG] Created 62500 chunks
Processing 62500 chunks...

Processing chunk 1/62500...
[DEBUG] Ollama: Sending request to qwen2.5:7b...
[DEBUG] Ollama: Using Metal/GPU acceleration (if available)
[DEBUG] Ollama: Received response
  âœ“ Structure updated

Processing chunk 2/62500...
...
```

## Optimization Tips

### 1. Close Background Apps
Free up RAM and GPU:
```bash
# Close unnecessary apps
# Safari with many tabs
# Slack, Discord
# Docker (if not needed)
```

### 2. Use Terminal in Low Power Mode
If running overnight:
```bash
# Prevent Mac from sleeping
caffeinate -i python -m airganizer.main /path --organize --output result.json
```

### 3. Run Overnight
Start before bed:
```bash
# With logging to file
python -m airganizer.main /path/to/data \
  --organize \
  --debug \
  --output structure.json \
  2>&1 | tee airganizer.log
```

Check progress in the morning!

## Estimated Timeline

**Day 1 (24 hours):**
- Process ~17,000 chunks (27% done)
- Can check progress anytime via temp file

**Day 2 (48 hours):**
- Process ~34,000 chunks (54% done)
- Halfway there!

**Day 3 (72 hours):**
- Process ~51,000 chunks (82% done)
- Almost done

**Day 4 (86 hours):**
- Complete! (~62,500 chunks processed)
- Total: **3.6 days**

## Troubleshooting

### If it's too slow:
Try smaller chunk size for faster individual responses:
```yaml
chunk_size: 50000  # ~5s per chunk, more chunks but faster per chunk
```

### If running out of memory:
Use an even smaller model:
```bash
ollama pull qwen2.5:3b  # Only ~2GB
```

```yaml
ollama:
  model: qwen2.5:3b
```

### If model crashes:
Increase available memory:
```bash
# Close all other apps
# Restart Ollama
ollama serve
```

## Alternative Models for 16GB RAM

If qwen2.5:7b doesn't work well:

| Model | Size | Context | Speed | Quality | Chunk Size |
|-------|------|---------|-------|---------|------------|
| qwen2.5:7b | 4.7GB | 32K | Fast | â­â­â­â­ | 80,000 |
| mistral:7b | 4.1GB | 32K | Fast | â­â­â­â­ | 80,000 |
| llama3.2:8b | 4.9GB | 128K | Medium | â­â­â­â­ | 100,000 |
| qwen2.5:3b | 2.0GB | 32K | Very Fast | â­â­â­ | 60,000 |

**Recommended: qwen2.5:7b** - Best balance for file organization.

## Final Command (Copy-Paste Ready)

```bash
# Setup (run once)
brew install ollama
ollama pull qwen2.5:7b

# Create config with these settings:
cat > .airganizer.yaml << 'EOF'
ai_provider: ollama
chunk_size: 80000
ollama:
  model: qwen2.5:7b
  base_url: http://localhost:11434
EOF

# Start Ollama
ollama serve &

# Run Airganizer (replace path with your actual data path)
python -m airganizer.main /path/to/your/650gb/data \
  --organize \
  --debug \
  --output organized_structure.json \
  2>&1 | tee airganizer.log
```

**Processing time: ~3.6 days (vs 16 days with current settings)**

ðŸš€ **4.4x faster, completely free, and private!**

# AI File Organizer Configuration File
# This is an example configuration file showing available options

# General settings
general:
  # Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  log_level: INFO
  
  # Maximum file size to process (in MB, 0 = no limit)
  max_file_size: 0
  
  # File extensions to exclude (comma-separated)
  exclude_extensions: []
  
  # Directories to exclude from scanning
  exclude_dirs: [".git", "__pycache__", "node_modules", ".venv"]

# Stage 1 settings
stage1:
  # Enable recursive directory scanning
  recursive: true
  
  # Follow symbolic links
  follow_symlinks: false
  
  # Include hidden files (files starting with .)
  include_hidden: false

# Cache settings
cache:
  # Enable/disable caching for resumable operations
  enabled: true
  
  # Cache directory location (relative or absolute path)
  directory: ".airganizer_cache"
  
  # Cache time-to-live in hours (entries older than this are considered stale)
  ttl_hours: 24

# AI Model configuration
models:
  # Model selection mode:
  # - "local_only": Use only local models (Ollama, etc.)
  # - "online_only": Use only online models (OpenAI, Anthropic, etc.)
  # - "mixed": Use both local and online models
  model_mode: "mixed"
  
  # Model discovery method:
  # - "auto": Automatically enumerate available models from providers
  # - "config": Use only models explicitly defined below
  # - "local_download": Auto-discover local models and download if needed
  discovery_method: "auto"
  
  # ============================================================================
  # PROVIDER CREDENTIALS (defined once)
  # ============================================================================
  
  # OpenAI Configuration
  openai:
    # API key (or set OPENAI_API_KEY environment variable)
    api_key_env: "OPENAI_API_KEY"
    api_key: ""  # Or specify directly (not recommended for security)
    
    # Auto-enumerate models from OpenAI API (when discovery_method is "auto")
    auto_enumerate: true
    
    # Optional: Specify which models to use (if empty, all available models are used)
    # Leave empty for automatic enumeration
    models: []
    # Example: Manually specify models
    # models:
    #   - "gpt-4-turbo-preview"
    #   - "gpt-4-vision-preview"
    #   - "gpt-3.5-turbo"
  
  # Anthropic Configuration
  anthropic:
    # API key (or set ANTHROPIC_API_KEY environment variable)
    api_key_env: "ANTHROPIC_API_KEY"
    api_key: ""  # Or specify directly (not recommended for security)
    
    # Auto-enumerate models from Anthropic API (when discovery_method is "auto")
    auto_enumerate: true
    
    # Optional: Specify which models to use
    models: []
    # Example:
    # models:
    #   - "claude-3-opus-20240229"
    #   - "claude-3-sonnet-20240229"
    #   - "claude-3-haiku-20240307"
  
  # Ollama Configuration (Local Models)
  ollama:
    # Ollama API base URL
    base_url: "http://localhost:11434"
    
    # Auto-enumerate locally available models
    auto_enumerate: true
    
    # Optional: Models to auto-download if not present (when discovery_method is "local_download")
    auto_download_models:
      - "llama3.2:latest"
      - "llava:latest"  # Vision model for images
    
    # Optional: Specify which local models to use
    models: []
    # Example:
    # models:
    #   - "llama3.2:latest"
    #   - "llava:latest"
    #   - "mistral:latest"
  
  # ============================================================================
  # MAPPING MODEL (AI Orchestrator)
  # ============================================================================
  # The AI model used to decide which model should analyze which file type
  mapping_model:
    provider: "openai"  # openai, anthropic, or ollama
    model_name: "gpt-4"  # Model to use for orchestration
    # Credentials are pulled from provider config above
  
  # ============================================================================
  # MANUAL MODEL DEFINITIONS (Optional)
  # ============================================================================
  # Only used when discovery_method is "config"
  # When using "auto" discovery, this section is ignored
  config_models:
    # Example: Manually define specific models with custom settings
    - name: "gpt-4-vision-custom"
      type: "online"
      provider: "ollama"
      model_name: "llama3.2:latest"
      capabilities:
        - "text"
      description: "Custom local model configuration"

# Future stages can be configured here
# stage2:
#   ...
